{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T08:26:41.963315Z","iopub.execute_input":"2025-09-27T08:26:41.963536Z","iopub.status.idle":"2025-09-27T08:26:42.226072Z","shell.execute_reply.started":"2025-09-27T08:26:41.963520Z","shell.execute_reply":"2025-09-27T08:26:42.225470Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/news-summary/news_summary_more.csv\n/kaggle/input/news-summary/news_summary.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T08:26:42.227260Z","iopub.execute_input":"2025-09-27T08:26:42.227653Z","iopub.status.idle":"2025-09-27T08:26:47.169278Z","shell.execute_reply.started":"2025-09-27T08:26:42.227628Z","shell.execute_reply":"2025-09-27T08:26:47.168145Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T08:26:47.170446Z","iopub.execute_input":"2025-09-27T08:26:47.170739Z","iopub.status.idle":"2025-09-27T08:27:01.375945Z","shell.execute_reply.started":"2025-09-27T08:26:47.170709Z","shell.execute_reply":"2025-09-27T08:27:01.375207Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nCollecting transformers\n  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\nSuccessfully installed huggingface-hub-0.35.1 tokenizers-0.22.1 transformers-4.56.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T08:27:01.377878Z","iopub.execute_input":"2025-09-27T08:27:01.378122Z","iopub.status.idle":"2025-09-27T08:27:06.613954Z","shell.execute_reply.started":"2025-09-27T08:27:01.378100Z","shell.execute_reply":"2025-09-27T08:27:06.613256Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1f0958b4692647972e06d553ece43890b5dae382f6f21654bf46cf0c3f1b9fa4\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import BartForConditionalGeneration, BartTokenizerFast\nfrom transformers import Trainer, TrainingArguments\n\n# 1. Load dataset\ndataset = load_dataset(\"csv\", data_files={\"dataset\": \"/kaggle/input/news-summary/news_summary.csv\"}, encoding=\"ISO-8859-1\")\ndataset = dataset['dataset'].train_test_split(test_size=0.1)\ntrain_dataset = dataset['train']\nval_dataset = dataset['test']\n\n# 2. Load tokenizer and model\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = BartTokenizerFast.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\n# 3. Tokenization function\ndef tokenize(batch):\n    # Convert everything to string to avoid None or other types\n    inputs = tokenizer([str(x) for x in batch['ctext']], \n                       max_length=512, truncation=True, padding=\"max_length\")\n    targets = tokenizer([str(x) for x in batch['text']], \n                        max_length=128, truncation=True, padding=\"max_length\")\n    \n    batch['input_ids'] = inputs['input_ids']\n    batch['attention_mask'] = inputs['attention_mask']\n    batch['labels'] = targets['input_ids']\n    return batch\n\n# Apply tokenization\nremove_cols = ['author', 'date', 'headlines', 'read_more', 'ctext', 'text']\ntokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=remove_cols)\ntokenized_val = val_dataset.map(tokenize, batched=True, remove_columns=remove_cols)\n\n\n# 4. Training arguments compatible with older transformers\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=3,\n    learning_rate=3e-5,\n    fp16=True,\n    logging_strategy=\"epoch\",     # <-- log at the end of each epoch\n    save_strategy=\"epoch\",        # <-- save checkpoint at end of each epoch\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# 5. ROUGE metric\nrouge = evaluate.load(\"rouge\")\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n    result = rouge.compute(predictions=pred_str, references=labels_str)\n    return {key: value * 100 for key, value in result.items()}\n\n# 6. Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# 7. Train\ntrainer.train()\n\n# 8. Save\nmodel.save_pretrained(\"./fine_tuned_bart_large_cnn\")\ntokenizer.save_pretrained(\"./fine_tuned_bart_large_cnn\")\n\nprint(\"Fine-tuning complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T08:27:06.614862Z","iopub.execute_input":"2025-09-27T08:27:06.615080Z","iopub.status.idle":"2025-09-27T09:25:10.834275Z","shell.execute_reply.started":"2025-09-27T08:27:06.615059Z","shell.execute_reply":"2025-09-27T09:25:10.833504Z"}},"outputs":[{"name":"stderr","text":"2025-09-27 08:27:17.067564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758961637.266743      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758961637.326986      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating dataset split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3961a404964fc09dc9f9846737d0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b9188eefec4f85bc3c0ab176c63af9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a994c4b1adaf474cb7fb12e2a8ddbdd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1276c137e5914f1ebee719ffaf9af936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f576474f374345cfaa608e1961a426be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b13d1be61d4bd988a0695884ade5bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9320bdb5ae414db0d4256d73d7f15e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4062 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6976c77a4d24b3690e8407bff9e3b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/452 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370105a5a8c0415b8c7def1f2f7558f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ea21d09f4d4288af0bdf2a7277699b"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/1327848807.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12186' max='12186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12186/12186 57:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>4062</td>\n      <td>0.987000</td>\n    </tr>\n    <tr>\n      <td>8124</td>\n      <td>0.601700</td>\n    </tr>\n    <tr>\n      <td>12186</td>\n      <td>0.361100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning complete!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Manual ROUGE evaluation (works regardless of transformers version)\nimport torch\nfrom transformers import DataCollatorForSeq2Seq\nimport evaluate\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Tune this for memory; 4 is usually safe on Kaggle T4\neval_batch_size = 4\n\n# collator pads to longest in batch and prepares tensors expected by model.generate\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"longest\", return_tensors=\"pt\")\n\n# Create a DataLoader over the tokenized validation dataset\neval_dataloader = DataLoader(tokenized_val, batch_size=eval_batch_size, collate_fn=data_collator)\n\npreds = []\nrefs = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        # move tensors to device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        # generate summaries\n        generated_ids = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=130,\n            min_length=30,\n            num_beams=4,\n            length_penalty=2.0,\n            early_stopping=True,\n            no_repeat_ngram_size=3\n        )\n\n        # decode predictions\n        batch_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        preds.extend(batch_preds)\n\n        # prepare references: replace -100 with pad_token_id, then decode\n        labels = batch[\"labels\"].detach().cpu().numpy()\n        labels[labels == -100] = tokenizer.pad_token_id\n        batch_refs = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        refs.extend(batch_refs)\n\n# compute ROUGE\nrouge = evaluate.load(\"rouge\")\nresult = rouge.compute(predictions=preds, references=refs)\nresult = {k: v * 100 for k, v in result.items()}   # percentage\nprint(\"ROUGE results (percent):\", result)\n\n# show 5 example pairs to inspect quality\nprint(\"\\nExamples (predicted -> reference):\\n\")\nfor i in range(5):\n    print(f\"Pred {i+1}: \", preds[i][:400])\n    print(f\"Ref  {i+1}: \", refs[i][:400])\n    print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:10.835302Z","iopub.execute_input":"2025-09-27T09:25:10.835913Z","iopub.status.idle":"2025-09-27T09:30:54.090281Z","shell.execute_reply.started":"2025-09-27T09:25:10.835893Z","shell.execute_reply":"2025-09-27T09:30:54.089437Z"}},"outputs":[{"name":"stdout","text":"ROUGE results (percent): {'rouge1': 49.867936895514795, 'rouge2': 27.5531305031589, 'rougeL': 37.26528886016578, 'rougeLsum': 37.22370285410269}\n\nExamples (predicted -> reference):\n\nPred 1:  Actor Dileep has been arrested in connection to the Malayalam actress abduction case. He was interrogated along with director Nadirshah for 13 hours to understand his involvement in the case, and his wife Kavya Madhavan's place of business was raided as well. Notably, actress Priyanka Chopra was abducted from the sets of her film Georgettan's Pooram in 2014.\nRef  1:  Actor Dileep was arrested on Monday in connection with the abduction and assault case of Malayalam actress, which took place in February 2017. Earlier, Dileep got linked to the case after his name was mentioned in a letter of a jail inmate, who shared the cell with the prime accused Pulsar Suni. Dileep has claimed that he doesn't know Suni.\n--------------------------------------------------------------------------------\nPred 2:  Australian pacer Mitchell Starc has said he might \"take Ravichandran Ashwin's advice and hit him on the badge\", after the Indian spinner rubbed his forehead with his index finger after dismissing him in the Bengaluru Test. \"I look forward to bowling to Ashwin in Australia. I might take his advice,\" Starc added. The Australian pacer had returned home injured after Australia's loss in Bengaluru.\nRef  2:  Australian pacer Mitchell Starc has said that he is looking forward to bowl to Ravichandran Ashwin in Australia and he might take Ashwin's advice and hit him on the badge. Starc was given a send-off by Ashwin during the second Test, with Ashwin tapping his forehead. It was in reply to Starc taunting Abhinav Mukund in the similar manner earlier.\n--------------------------------------------------------------------------------\nPred 3:  The National Highways Authority of India (NHAI) has chopped over 8,000 trees in Gurugram to develop a flyover and three underpasses, according to reports. \"Trees were chopped mercilessly and many of these were fullgrown and more than 25 years old,\" said an environmentalist. Meteorological department officials have also raised concerns about possible increase in temperature by 2-3°C.\nRef  3:  The National Highways Authority of India (NHAI) has chopped more than 8,000 trees in Gurugram to develop a flyover and three underpasses. While environmentalists have criticised NHAI for the move in the name of development, meteorological department officials have raised concerns about a possible increase in temperature by 2-3 degrees.\n--------------------------------------------------------------------------------\nPred 4:  A 69-year-old man, Gopal Ansal, surrendered before Tihar jail authorities to undergo a one-year jail term in the 1997 Uphaar fire tragedy case after the Supreme Court refused to grant him more time to surrender. Ansal was in jail earlier for around four-and-a-half-month in connection with the case in which 59 people died in a stampede during the screening of a Hindi movie.\nRef  4:  Real estate baron Gopal Ansal on Monday surrendered before Tihar jail authorities to face one-year jail term in the 1997 Uphaar fire tragedy case. This comes after the Supreme Court rejected his plea seeking more time to surrender. Notably, 59 persons had died in Uphaar Cinema in Delhi during the screening of Hindi movie 'Border' on June 13, 1997.\n--------------------------------------------------------------------------------\nPred 5:  Ex-gratia compensation for paramilitary personnel, injured in action leading to 100% disability, has been increased from?9 lakh to?20 lakh. The enhanced compensation will be applicable to all central paramilitary personnel whose disability is attributed to or aggravated in service on or after January 1, 2016, the home ministry said. About 10 lakh personnel in eight forces come under the Home Minis\nRef  5:  The Home Ministry has more than doubled the compensation for paramilitary personnel suffering 100% disability in action, increasing it from?9 lakh to?20 lakh. This will be applicable to personnel whose disability is attributed to or aggravated in service from January 1, 2016. The new policy will cover 10 lakh personnel in eight forces under the home ministry.\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizerFast\nimport torch\n\n# Load the saved model\nmodel_path = \"./fine_tuned_bart_large_cnn\"\ntokenizer = BartTokenizerFast.from_pretrained(model_path)\nmodel = BartForConditionalGeneration.from_pretrained(model_path)\nmodel.eval()  # Set model to evaluation mode\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:30:54.091278Z","iopub.execute_input":"2025-09-27T09:30:54.091762Z","iopub.status.idle":"2025-09-27T09:30:54.525346Z","shell.execute_reply.started":"2025-09-27T09:30:54.091740Z","shell.execute_reply":"2025-09-27T09:30:54.524580Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def generate_summary(text, max_input_len=512, max_output_len=128):\n    # Tokenize input\n    inputs = tokenizer(\n        text,\n        max_length=max_input_len,\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n\n    # Generate summary IDs\n    summary_ids = model.generate(\n        inputs[\"input_ids\"],\n        num_beams=4,\n        max_length=max_output_len,\n        min_length=30,\n        length_penalty=2.0,\n        early_stopping=True\n    )\n\n    # Decode output properly\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:35:54.390874Z","iopub.execute_input":"2025-09-27T09:35:54.391191Z","iopub.status.idle":"2025-09-27T09:35:54.396805Z","shell.execute_reply.started":"2025-09-27T09:35:54.391169Z","shell.execute_reply":"2025-09-27T09:35:54.395981Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def generate_summary(text, max_input_len=512, max_new_tokens=128, min_output_len=10):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n\n    # Tokenize input and move to same device\n    inputs = tokenizer(\n        text,\n        max_length=max_input_len,\n        truncation=True,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    # Generate summary with relaxed settings\n    summary_ids = model.generate(\n        inputs[\"input_ids\"],\n        num_beams=4,\n        max_new_tokens=max_new_tokens,\n        min_length=min_output_len,\n        length_penalty=1.0,       # less aggressive than 2.0\n        early_stopping=False,     # safer for short texts\n        no_repeat_ngram_size=2    # avoid repeating phrases\n    )\n\n    # Decode\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary.strip() if summary else \"⚠️ No summary generated\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:44:47.266378Z","iopub.execute_input":"2025-09-27T09:44:47.266843Z","iopub.status.idle":"2025-09-27T09:44:47.273778Z","shell.execute_reply.started":"2025-09-27T09:44:47.266814Z","shell.execute_reply":"2025-09-27T09:44:47.272976Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"article = \"\"\"\nScientists at the European Organization for Nuclear Research (CERN) have announced a breakthrough in particle physics after completing new experiments at the Large Hadron Collider (LHC). The experiments aimed to explore the behavior of subatomic particles under extreme conditions, providing insights into the fundamental forces of the universe. Researchers reported observing rare particle interactions that were previously theoretical, potentially shedding light on unanswered questions in quantum mechanics and the Standard Model of particle physics.\n\nThe LHC, the world’s largest and most powerful particle accelerator, has been operational since 2008 and continues to provide unprecedented data for physicists around the globe. In the latest experiments, protons were accelerated to near-light speeds and collided, producing a range of exotic particles, some of which have never been detected before. These findings could have implications for understanding dark matter, antimatter, and the early moments of the universe following the Big Bang.\n\nCERN scientists emphasize that while these results are preliminary, they offer promising avenues for future research. International teams are already planning follow-up experiments to replicate and expand upon these observations. Beyond the theoretical implications, the technologies developed for these experiments are expected to benefit fields like medical imaging, materials science, and information technology. The scientific community is eagerly awaiting peer-reviewed publications to validate these groundbreaking discoveries.\n\n\"\"\"\n\nsummary = generate_summary(article)\nprint(\"Generated Summary:\\n\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:46:25.032042Z","iopub.execute_input":"2025-09-27T09:46:25.032365Z","iopub.status.idle":"2025-09-27T09:46:26.244506Z","shell.execute_reply.started":"2025-09-27T09:46:25.032342Z","shell.execute_reply":"2025-09-27T09:46:26.243853Z"}},"outputs":[{"name":"stdout","text":"Generated Summary:\n Scientists at the European Organization for Nuclear Research (CERN) have announced a breakthrough in particle physics after completing new experiments at its Large Hadron Collider (LHC). The experiments aimed to explore the behavior of subatomic particles under extreme conditions, providing insights into the fundamental forces of the universe. Researchers reported observing rare particle interactions that were previously theoretical, potentially shedding light on unanswered questions in quantum mechanics.\n","output_type":"stream"}],"execution_count":26}]}